{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#downloading the required data and resources\n",
        "nltk.download('all')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkAPvWBCVvnA",
        "outputId": "19ce59e7-fe40-4e1a-bb67-68219e1e0472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4yc5_lJYogg"
      },
      "outputs": [],
      "source": [
        "#University Project\n",
        "\n",
        "import math\n",
        "# imports the entire nltk library\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# imports only the sent_tokenize function from the nltk.tokenize module.\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# nlargest return the n largest elements from an iterable.\n",
        "# imports the nlargest function from the heapq module.\n",
        "import heapq\n",
        "from heapq import nlargest\n",
        "\n",
        "#import the WordNetLemmatizer class from the nltk.stem module. \n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#creates a set of stop words in English language using the stopwords module\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "#The WordNetLemmatizer class can be used to lemmatize individual words, lists of words, or entire sentences.\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to clean the text\n",
        "def clean_text(text):\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub('[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "    # Lemmatize the words\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    # Remove the stop words from the words list\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in lemmatized_words if word.casefold() not in stop_words]\n",
        "    # Combine the filtered words into a cleaned text\n",
        "    cleaned_text = ' '.join(filtered_words)\n",
        "    return cleaned_text\n",
        "\n",
        "def summarize(text):\n",
        "    # Clean the text\n",
        "    cleaned_text = clean_text(text)\n",
        "    # Tokenize the cleaned text into sentences\n",
        "    sentences = sent_tokenize(cleaned_text)\n",
        "    \n",
        "    # Calculate the frequency of each word\n",
        "    word_frequencies = nltk.FreqDist(cleaned_text.split())\n",
        "    \n",
        "    # Calculate the score of each sentence\n",
        "    sentence_scores = {}\n",
        "    for sentence in sentences:\n",
        "        for word in word_tokenize(sentence):\n",
        "            if word.casefold() in word_frequencies:\n",
        "                if sentence not in sentence_scores:\n",
        "                    sentence_scores[sentence] = word_frequencies[word.casefold()]\n",
        "                else:\n",
        "                    sentence_scores[sentence] += word_frequencies[word.casefold()]\n",
        "                    \n",
        "    # Get the top n sentences with the highest scores\n",
        "    summary_sentences = nlargest(5, sentence_scores, key=sentence_scores.get)\n",
        "    \n",
        "    # Combine the summary sentences into a summary\n",
        "    summary = ' '.join(summary_sentences)\n",
        "    return summary\n",
        "\n",
        "# Function to generate frequency-based summary\n",
        "def freq_summarize(text, n=5):\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    # Remove stop words and tokenize words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word.lower() for sentence in sentences for word in word_tokenize(sentence) if word.lower() not in stop_words]\n",
        "    # Calculate the frequency of each word\n",
        "    word_freq = dict()\n",
        "    for word in words:\n",
        "        if word in word_freq:\n",
        "            word_freq[word] += 1\n",
        "        else:\n",
        "            word_freq[word] = 1\n",
        "    # Normalize word frequency by dividing by maximum frequency\n",
        "    max_freq = max(word_freq.values())\n",
        "    for word in word_freq.keys():\n",
        "        word_freq[word] /= max_freq\n",
        "    # Create a TF-IDF matrix of the sentences\n",
        "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
        "    # Calculate the sentence scores based on TF-IDF values\n",
        "    sentence_scores = dict()\n",
        "    for i in range(len(sentences)):\n",
        "        score = 0\n",
        "        for word in word_tokenize(sentences[i]):\n",
        "            if word.lower() in word_freq.keys():\n",
        "                score += word_freq[word.lower()]\n",
        "        sentence_scores[i] = score\n",
        "    # Select the top n sentences based on scores\n",
        "    summary_sentences = heapq.nlargest(n, sentence_scores, key=sentence_scores.get)\n",
        "    summary = [sentences[i] for i in summary_sentences]\n",
        "    return ' '.join(summary)\n",
        "\n",
        "\n",
        "# Function to generate position-based summary\n",
        "def pos_summarize(text, num_sentences=3):\n",
        "    # Clean the text\n",
        "    cleaned_text = clean_text(text)\n",
        "    # Tokenize the cleaned text into sentences\n",
        "    sentences = sent_tokenize(cleaned_text)\n",
        "    # Calculate the position of each sentence in the text\n",
        "    sentence_positions = [(i+1)/len(sentences) for i in range(len(sentences))]\n",
        "    \n",
        "    # Get the top n sentences with the highest scores\n",
        "    top_sentences = heapq.nlargest(num_sentences, sentences, key=lambda s: sentence_positions[sentences.index(s)])\n",
        "    \n",
        "    # Combine the summary sentences into a summary\n",
        "    summary = ' '.join(top_sentences)\n",
        "    return summary\n",
        "\n",
        "# Example use cases:\n",
        "\n",
        "#Test Case 1\n",
        "#News article\n",
        "news_article = \"\"\"As the COVID-19 pandemic continues to grip the world, scientists are working around the clock to develop vaccines that can protect people from the virus. Researchers at several leading pharmaceutical companies have developed vaccines that have been shown to be highly effective in clinical trials. However, challenges remain in terms of producing and distributing these vaccines on a global scale.\"\"\"\n",
        "\n",
        "# Summarize of news article\n",
        "# Generate the frequency-based summary\n",
        "freq_summary_news_article = freq_summarize(news_article)\n",
        "print(\"Test Case 1 - News article\\nFrequency-based summary:\\n\", freq_summary_news_article)\n",
        "\n",
        "# Generate the position-based summary\n",
        "pos_summary_news_article = pos_summarize(news_article)\n",
        "print(\"\\nPosition-based summary:\\n\", pos_summary_news_article)\n",
        "\n",
        "\n",
        "#Test Case 2\n",
        "#Legal document\n",
        "legal_document = \"\"\"This agreement, entered into by and between the parties here to, sets forth the terms and conditions under which the parties shall conduct their business. The parties agree to be bound by the terms and conditions of this agreement, and acknowledge that any breach of this agreement may result in damages to the non-breaching party.\"\"\"\n",
        "\n",
        "# Summarize the legal document\n",
        "# Generate the frequency-based summary\n",
        "freq_summary_legal_document = freq_summarize(legal_document)\n",
        "print(\"\\n\\n\\nTest Case 2 - Legal Document\\nFrequency-based summary:\\n\", freq_summary_legal_document)\n",
        "\n",
        "# Generate the position-based summary\n",
        "pos_summary_legal_document = pos_summarize(legal_document)\n",
        "print(\"\\nPosition-based summary:\\n\", pos_summary_legal_document)\n",
        "\n",
        "\n",
        "#Test Case 3\n",
        "#Scientific paper\n",
        "scientific_paper = \"\"\"In this study, we investigate the effects of a new drug on patients with a rare genetic disorder. Our results show that the drug is highly effective in reducing symptoms of the disorder, and that it is well-tolerated by patients. These findings have important implications for the development of new treatments for rare genetic disorders.\"\"\"\n",
        "\n",
        "# Summarize the scientific paper\n",
        "# Generate the frequency-based summary\n",
        "freq_summary_scientific_paper = freq_summarize(scientific_paper)\n",
        "print(\"\\n\\n\\nTest Case 3 - Scientific paper\\nFrequency-based summary:\\n\", freq_summary_scientific_paper)\n",
        "\n",
        "# Generate the position-based summary\n",
        "pos_summary_scientific_paper = pos_summarize(scientific_paper)\n",
        "print(\"\\nPosition-based summary:\\n\", pos_summary_scientific_paper)\n",
        "\n",
        "\n",
        "\n",
        "article = \"Peter and Elizabeth took a taxi to attend the night party in the city. While in the party, Elizabeth collapsed and WAS rushed to the hospital\"\n",
        "\n",
        "freq_summary_article = freq_summarize(article)\n",
        "print(\"\\n\\nSample Case - \\nFrequency-based summary:\\n\", freq_summary_article)\n",
        "\n",
        "# Generate the position-based summary\n",
        "pos_summary_article = pos_summarize(article)\n",
        "print(\"\\nPosition-based summary:\\n\", pos_summary_article)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiXAL_x5atIU",
        "outputId": "f113df27-b1a6-41f7-9663-f55f7c9528ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Case 1 - News article\n",
            "Frequency-based summary:\n",
            " As the COVID-19 pandemic continues to grip the world, scientists are working around the clock to develop vaccines that can protect people from the virus. Researchers at several leading pharmaceutical companies have developed vaccines that have been shown to be highly effective in clinical trials. However, challenges remain in terms of producing and distributing these vaccines on a global scale.\n",
            "\n",
            "Position-based summary:\n",
            " COVID pandemic continues grip world scientist working around clock develop vaccine protect people virus Researchers several leading pharmaceutical company developed vaccine shown highly effective clinical trial However challenge remain term producing distributing vaccine global scale\n",
            "\n",
            "\n",
            "\n",
            "Test Case 2 - Legal Document\n",
            "Frequency-based summary:\n",
            " This agreement, entered into by and between the parties here to, sets forth the terms and conditions under which the parties shall conduct their business. The parties agree to be bound by the terms and conditions of this agreement, and acknowledge that any breach of this agreement may result in damages to the non-breaching party.\n",
            "\n",
            "Position-based summary:\n",
            " agreement entered party set forth term condition party shall conduct business party agree bound term condition agreement acknowledge breach agreement may result damage nonbreaching party\n",
            "\n",
            "\n",
            "\n",
            "Test Case 3 - Scientific paper\n",
            "Frequency-based summary:\n",
            " In this study, we investigate the effects of a new drug on patients with a rare genetic disorder. Our results show that the drug is highly effective in reducing symptoms of the disorder, and that it is well-tolerated by patients. These findings have important implications for the development of new treatments for rare genetic disorders.\n",
            "\n",
            "Position-based summary:\n",
            " study investigate effect new drug patient rare genetic disorder result show drug highly effective reducing symptom disorder welltolerated patient finding important implication development new treatment rare genetic disorder\n",
            "\n",
            "\n",
            "Sample Case - \n",
            "Frequency-based summary:\n",
            " Peter and Elizabeth took a taxi to attend the night party in the city. While in the party, Elizabeth collapsed and WAS rushed to the hospital\n",
            "\n",
            "Position-based summary:\n",
            " Peter Elizabeth took taxi attend night party city party Elizabeth collapsed rushed hospital\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _create_frequency_table(text_string) -> dict:\n",
        "    \"\"\"\n",
        "    we create a dictionary for the word frequency table.\n",
        "    For this, we should only use the words that are not part of the stopWords array.\n",
        "    Removing stop words and making frequency table\n",
        "    Stemmer - an algorithm to bring words to its root word.\n",
        "    :rtype: dict\n",
        "    \"\"\"\n",
        "    stopWords = set(stopwords.words(\"english\"))\n",
        "    words = word_tokenize(text_string)\n",
        "    ps = PorterStemmer()\n",
        "\n",
        "    freqTable = dict()\n",
        "    for word in words:\n",
        "        word = ps.stem(word)\n",
        "        if word in stopWords:\n",
        "            continue\n",
        "        if word in freqTable:\n",
        "            freqTable[word] += 1\n",
        "        else:\n",
        "            freqTable[word] = 1\n",
        "\n",
        "    return freqTable\n",
        "\n",
        "\n",
        "def _create_frequency_matrix(sentences):\n",
        "    frequency_matrix = {}\n",
        "    stopWords = set(stopwords.words(\"english\"))\n",
        "    ps = PorterStemmer()\n",
        "\n",
        "    for sent in sentences:\n",
        "        freq_table = {}\n",
        "        words = word_tokenize(sent)\n",
        "        for word in words:\n",
        "            word = word.lower()\n",
        "            word = ps.stem(word)\n",
        "            if word in stopWords:\n",
        "                continue\n",
        "\n",
        "            if word in freq_table:\n",
        "                freq_table[word] += 1\n",
        "            else:\n",
        "                freq_table[word] = 1\n",
        "\n",
        "        frequency_matrix[sent[:15]] = freq_table\n",
        "\n",
        "    return frequency_matrix\n",
        "\n",
        "\n",
        "def _create_tf_matrix(freq_matrix):\n",
        "    tf_matrix = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        tf_table = {}\n",
        "\n",
        "        count_words_in_sentence = len(f_table)\n",
        "        for word, count in f_table.items():\n",
        "            tf_table[word] = count / count_words_in_sentence\n",
        "\n",
        "        tf_matrix[sent] = tf_table\n",
        "\n",
        "    return tf_matrix\n",
        "\n",
        "\n",
        "def _create_documents_per_words(freq_matrix):\n",
        "    word_per_doc_table = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        for word, count in f_table.items():\n",
        "            if word in word_per_doc_table:\n",
        "                word_per_doc_table[word] += 1\n",
        "            else:\n",
        "                word_per_doc_table[word] = 1\n",
        "\n",
        "    return word_per_doc_table\n",
        "\n",
        "\n",
        "def _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
        "    idf_matrix = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        idf_table = {}\n",
        "\n",
        "        for word in f_table.keys():\n",
        "            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
        "\n",
        "        idf_matrix[sent] = idf_table\n",
        "\n",
        "    return idf_matrix\n",
        "\n",
        "\n",
        "def _create_tf_idf_matrix(tf_matrix, idf_matrix):\n",
        "    tf_idf_matrix = {}\n",
        "\n",
        "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
        "\n",
        "        tf_idf_table = {}\n",
        "\n",
        "        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n",
        "                                                    f_table2.items()):  # here, keys are the same in both the table\n",
        "            tf_idf_table[word1] = float(value1 * value2)\n",
        "\n",
        "        tf_idf_matrix[sent1] = tf_idf_table\n",
        "\n",
        "    return tf_idf_matrix\n",
        "\n",
        "\n",
        "def _score_sentences(tf_idf_matrix) -> dict:\n",
        "    \"\"\"\n",
        "    score a sentence by its word's TF\n",
        "    Basic algorithm: adding the TF frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n",
        "    :rtype: dict\n",
        "    \"\"\"\n",
        "\n",
        "    sentenceValue = {}\n",
        "\n",
        "    for sent, f_table in tf_idf_matrix.items():\n",
        "        total_score_per_sentence = 0\n",
        "\n",
        "        count_words_in_sentence = len(f_table)\n",
        "        for word, score in f_table.items():\n",
        "            total_score_per_sentence += score\n",
        "\n",
        "        sentenceValue[sent] = total_score_per_sentence / count_words_in_sentence\n",
        "\n",
        "    return sentenceValue\n",
        "\n",
        "\n",
        "def _find_average_score(sentenceValue) -> int:\n",
        "    \"\"\"\n",
        "    Find the average score from the sentence value dictionary\n",
        "    :rtype: int\n",
        "    \"\"\"\n",
        "    sumValues = 0\n",
        "    for entry in sentenceValue:\n",
        "        sumValues += sentenceValue[entry]\n",
        "\n",
        "    # Average value of a sentence from original summary_text\n",
        "    average = (sumValues / len(sentenceValue))\n",
        "\n",
        "    return average\n",
        "\n",
        "\n",
        "def _generate_summary(sentences, sentenceValue, threshold):\n",
        "    sentence_count = 0\n",
        "    summary = ''\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sentence[:15] in sentenceValue and sentenceValue[sentence[:15]] >= (threshold):\n",
        "            summary += \" \" + sentence\n",
        "            sentence_count += 1\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "def run_summarization(text):\n",
        "    \"\"\"\n",
        "    :param text: Plain summary_text of long article\n",
        "    :return: summarized summary_text\n",
        "    \"\"\"\n",
        "\n",
        "    '''\n",
        "    We already have a sentence tokenizer, so we just need \n",
        "    to run the sent_tokenize() method to create the array of sentences.\n",
        "    '''\n",
        "    # 1 Sentence Tokenize\n",
        "    sentences = sent_tokenize(text)\n",
        "    total_documents = len(sentences)\n",
        "    #print(sentences)\n",
        "\n",
        "    # 2 Create the Frequency matrix of the words in each sentence.\n",
        "    freq_matrix = _create_frequency_matrix(sentences)\n",
        "    #print(freq_matrix)\n",
        "\n",
        "    '''\n",
        "    Term frequency (TF) is how often a word appears in a document, divided by how many words are there in a document.\n",
        "    '''\n",
        "    # 3 Calculate TermFrequency and generate a matrix\n",
        "    tf_matrix = _create_tf_matrix(freq_matrix)\n",
        "    #print(tf_matrix)\n",
        "\n",
        "    # 4 creating table for documents per words\n",
        "    count_doc_per_words = _create_documents_per_words(freq_matrix)\n",
        "    #print(count_doc_per_words)\n",
        "\n",
        "    '''\n",
        "    Inverse document frequency (IDF) is how unique or rare a word is.\n",
        "    '''\n",
        "    # 5 Calculate IDF and generate a matrix\n",
        "    idf_matrix = _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n",
        "    #print(idf_matrix)\n",
        "\n",
        "    # 6 Calculate TF-IDF and generate a matrix\n",
        "    tf_idf_matrix = _create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
        "    #print(tf_idf_matrix)\n",
        "\n",
        "    # 7 Important Algorithm: score the sentences\n",
        "    sentence_scores = _score_sentences(tf_idf_matrix)\n",
        "    #print(sentence_scores)\n",
        "\n",
        "    # 8 Find the threshold\n",
        "    threshold = _find_average_score(sentence_scores)\n",
        "    #print(threshold)\n",
        "\n",
        "    # 9 Important Algorithm: Generate the summary\n",
        "    summary = _generate_summary(sentences, sentence_scores, 1.3 * threshold)\n",
        "    return summary\n",
        "\n",
        "# Function to generate frequency-based summary\n",
        "def freq_summarize(text, n=5):\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    # Remove stop words and tokenize words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word.lower() for sentence in sentences for word in word_tokenize(sentence) if word.lower() not in stop_words]\n",
        "    # Calculate the frequency of each word\n",
        "    word_freq = dict()\n",
        "    for word in words:\n",
        "        if word in word_freq:\n",
        "            word_freq[word] += 1\n",
        "        else:\n",
        "            word_freq[word] = 1\n",
        "    # Normalize word frequency by dividing by maximum frequency\n",
        "    max_freq = max(word_freq.values())\n",
        "    for word in word_freq.keys():\n",
        "        word_freq[word] /= max_freq\n",
        "    # Create a TF-IDF matrix of the sentences\n",
        "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
        "    # Calculate the sentence scores based on TF-IDF values\n",
        "    sentence_scores = dict()\n",
        "    for i in range(len(sentences)):\n",
        "        score = 0\n",
        "        for word in word_tokenize(sentences[i]):\n",
        "            if word.lower() in word_freq.keys():\n",
        "                score += word_freq[word.lower()]\n",
        "        sentence_scores[i] = score\n",
        "    # Select the top n sentences based on scores\n",
        "    summary_sentences = heapq.nlargest(n, sentence_scores, key=sentence_scores.get)\n",
        "    summary = [sentences[i] for i in summary_sentences]\n",
        "    return ' '.join(summary)\n",
        "\n",
        "text_str = '''\n",
        "Those Who Are Resilient Stay In The Game Longer\n",
        "“On the mountains of truth you can never climb in vain: either you will reach a point higher up today, or you will be training your powers so that you will be able to climb higher tomorrow.” — Friedrich Nietzsche\n",
        "Challenges and setbacks are not meant to defeat you, but promote you. However, I realise after many years of defeats, it can crush your spirit and it is easier to give up than risk further setbacks and disappointments. Have you experienced this before? To be honest, I don’t have the answers. I can’t tell you what the right course of action is; only you will know. However, it’s important not to be discouraged by failure when pursuing a goal or a dream, since failure itself means different things to different people. To a person with a Fixed Mindset failure is a blow to their self-esteem, yet to a person with a Growth Mindset, it’s an opportunity to improve and find new ways to overcome their obstacles. Same failure, yet different responses. Who is right and who is wrong? Neither. Each person has a different mindset that decides their outcome. Those who are resilient stay in the game longer and draw on their inner means to succeed.\n",
        "I’ve coached mummy and mom clients who gave up after many years toiling away at their respective goal or dream. It was at that point their biggest breakthrough came. Perhaps all those years of perseverance finally paid off. It was the 19th Century’s minister Henry Ward Beecher who once said: “One’s best success comes after their greatest disappointments.” No one knows what the future holds, so your only guide is whether you can endure repeated defeats and disappointments and still pursue your dream. Consider the advice from the American academic and psychologist Angela Duckworth who writes in Grit: The Power of Passion and Perseverance: “Many of us, it seems, quit what we start far too early and far too often. Even more than the effort a gritty person puts in on a single day, what matters is that they wake up the next day, and the next, ready to get on that treadmill and keep going.”\n",
        "I know one thing for certain: don’t settle for less than what you’re capable of, but strive for something bigger. Some of you reading this might identify with this message because it resonates with you on a deeper level. For others, at the end of their tether the message might be nothing more than a trivial pep talk. What I wish to convey irrespective of where you are in your journey is: NEVER settle for less. If you settle for less, you will receive less than you deserve and convince yourself you are justified to receive it.\n",
        "“Two people on a precipice over Yosemite Valley” by Nathan Shipps on Unsplash\n",
        "Develop A Powerful Vision Of What You Want\n",
        "“Your problem is to bridge the gap which exists between where you are now and the goal you intend to reach.” — Earl Nightingale\n",
        "I recall a passage my father often used growing up in 1990s: “Don’t tell me your problems unless you’ve spent weeks trying to solve them yourself.” That advice has echoed in my mind for decades and became my motivator. Don’t leave it to other people or outside circumstances to motivate you because you will be let down every time. It must come from within you. Gnaw away at your problems until you solve them or find a solution. Problems are not stop signs, they are advising you that more work is required to overcome them. Most times, problems help you gain a skill or develop the resources to succeed later. So embrace your challenges and develop the grit to push past them instead of retreat in resignation. Where are you settling in your life right now? Could you be you playing for bigger stakes than you are? Are you willing to play bigger even if it means repeated failures and setbacks? You should ask yourself these questions to decide whether you’re willing to put yourself on the line or settle for less. And that’s fine if you’re content to receive less, as long as you’re not regretful later.\n",
        "If you have not achieved the success you deserve and are considering giving up, will you regret it in a few years or decades from now? Only you can answer that, but you should carve out time to discover your motivation for pursuing your goals. It’s a fact, if you don’t know what you want you’ll get what life hands you and it may not be in your best interest, affirms author Larry Weidel: “Winners know that if you don’t figure out what you want, you’ll get whatever life hands you.” The key is to develop a powerful vision of what you want and hold that image in your mind. Nurture it daily and give it life by taking purposeful action towards it.\n",
        "Vision + desire + dedication + patience + daily action leads to astonishing success. Are you willing to commit to this way of life or jump ship at the first sign of failure? I’m amused when I read questions written by millennials on Quora who ask how they can become rich and famous or the next Elon Musk. Success is a fickle and long game with highs and lows. Similarly, there are no assurances even if you’re an overnight sensation, to sustain it for long, particularly if you don’t have the mental and emotional means to endure it. This means you must rely on the one true constant in your favour: your personal development. The more you grow, the more you gain in terms of financial resources, status, success — simple. If you leave it to outside conditions to dictate your circumstances, you are rolling the dice on your future.\n",
        "So become intentional on what you want out of life. Commit to it. Nurture your dreams. Focus on your development and if you want to give up, know what’s involved before you take the plunge. Because I assure you, someone out there right now is working harder than you, reading more books, sleeping less and sacrificing all they have to realise their dreams and it may contest with yours. Don’t leave your dreams to chance.\n",
        "'''\n",
        "text_str1 = \"Hi, my name is Sumit and I'm a college student. My hobbies include watching and playing football, analysing and developing data science projects, and I also sleep a lot. Right now I'm planning to go for my master's degree and then keep doing jobs untill I implement my startup ideas. First I want to learn to be an employee and see how corporate companies work and then become an employer. Most of the time I've my earphones plugged in my ears and the music is playing non-stop. I also love spending my time at the gym but due to college and studies I'm not able to add it in my daily schedule. Apart from all this, I like playing pin bowling as well.\"\n",
        "\n",
        "#result = run_summarization(text_str)\n",
        "#print(\"\\n\\n\",result)\n",
        "\n",
        "print(freq_summarize(text_str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQSaoAXiVBAj",
        "outputId": "6ccebe56-7d8b-46f5-951c-b791b89d303c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My hobbies include watching and playing football, analysing and developing data science projects, and I also sleep a lot. I also love spending my time at the gym but due to college and studies I'm not able to add it in my daily schedule. Right now I'm planning to go for my master's degree and then keep doing jobs untill I implement my startup ideas. Hi, my name is Sumit and I'm a college student. Apart from all this, I like playing pin bowling as well.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UX7szsBlGQZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRyl1bahGZdk",
        "outputId": "e296f79c-8fcb-4dcc-c5ed-3681df4e797b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the pre-trained summarization model\n",
        "summarizer = pipeline(\"summarization\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CN1pP_GGRHx",
        "outputId": "113d6b04-e6ec-41e9-decd-403c01a64ebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input text to be summarized\n",
        "input_text = \"\"\" The Amazon rainforest is the world's largest tropical rainforest, covering over five and a half million square kilometers. It is home to millions of species of plants and animals, many of which are found nowhere else on earth. However, the rainforest is under threat from deforestation, which is driven by logging, agriculture, and other human activities. This destruction of the rainforest is having a devastating impact on both the environment and the indigenous communities who call it home. \"\"\"\n",
        "\n",
        "# Generate a summary of the input text\n",
        "summary = summarizer(input_text, max_length=100, min_length=30, do_sample=False)\n",
        "\n",
        "# Print the summary\n",
        "print(summary[0]['summary_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ocs7iHk_HRwq",
        "outputId": "e21c0afd-6533-4133-886e-055e0d427a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 100, but you input_length is only 97. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The Amazon rainforest is the world's largest tropical rainforest, covering over five and a half million square kilometers . It is home to millions of species of plants and animals, many of which are found nowhere else on earth . However, deforestation is under threat from logging, agriculture, and other human activities .\n"
          ]
        }
      ]
    }
  ]
}